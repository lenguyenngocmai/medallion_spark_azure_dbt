{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "", --add the key
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/bronze has been unmounted.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the source information\n",
    "account_name = \"medallion1\"\n",
    "container_name = \"bronze\"\n",
    "folder_path = \"20240118\"   # <-- Set the name of your Parquet folder here\n",
    "\n",
    "# Provide your storage account key \n",
    "storage_account_access_key = \"+Z7unWQrXCsx8I2j1TggARQXoFA0vWOeSGD4YilMjopyDkoPX+E9SuJKMk6DCo+mW7Yh/CM69VEx+AStgJIQ4Q==\" # Replace with your key\n",
    "\n",
    "# Unmount the directory if it is already mounted\n",
    "if f\"/mnt/{container_name}\" in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:\n",
    "  dbutils.fs.unmount(f\"/mnt/{container_name}\")\n",
    "\n",
    "# Mount the Blob Storage account to a DBFS directory with credentials\n",
    "dbutils.fs.mount(\n",
    "  source=f\"wasbs://{container_name}@{account_name}.blob.core.windows.net/\",\n",
    "  mount_point=f\"/mnt/{container_name}\",\n",
    "  extra_configs={\n",
    "    f\"fs.azure.account.key.{account_name}.blob.core.windows.net\":storage_account_access_key\n",
    "  }\n",
    ")\n",
    "\n",
    "# List all Parquet files in the specified folder on the Blob Storage account\n",
    "file_paths = [f\"/mnt/{container_name}/{folder_path}/{file.name}\"\n",
    "              for file in dbutils.fs.ls(f\"/mnt/{container_name}/{folder_path}\")\n",
    "             if file.isFile() and file.name.endswith(\".parquet\")]\n",
    "             \n",
    "# Create the database using spark.sql()\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "\n",
    "# Load each Parquet file into a separate DataFrame in Databricks\n",
    "for file_path in file_paths:\n",
    "    # Derive the table name from the file name\n",
    "    table_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df = spark.read.parquet(file_path) \n",
    "    # Create table using Hive metastore\n",
    "    df.write.format(\"parquet\").saveAsTable(f\"bronze.{table_name}\")\n",
    "\n",
    "# Unmount the Blob Storage account om the DBFS directory\n",
    "dbutils.fs.unmount(f\"/mnt/{container_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfe12d6-d9c2-44b2-bc4d-bcc187d49bcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP SCHEMA IF EXISTS mydatabase CASCADE;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1320840667514218,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "base_notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
